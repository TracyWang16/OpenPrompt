dataset:
  name: webnlg
  path: ./datasets/CondGen/webnlg_2017

task: generation

train:
  num_epochs: 5
  batch_size: 5
  teacher_forcing: True
  gradient_accumulation_step: 1

dataloader:
  max_seq_length: 160
  
generation: # Adding any arguments for generation here.
  parent_config: task
  max_length: 320

plm:
  model_name: gpt2
  model_path: gpt2-medium
  optimize: 
    freeze_para: True

## LEARINING SETTING  ####################################################
learning_setting: full # selecting from "full", "zero-shot", "few-shot"

# few_shot:
#   parent_config: learning_setting
#   few_shot_sampling: sampling_from_train
  
# sampling_from_train:
#   parent_config: few_shot_sampling
#   num_examples_per_label: 100
#   also_sample_dev: True
#   num_examples_per_label_dev: 100
#   seed: 123


template: prefix_tuning_template
verbalizer: 

prefix_tuning_template:
  parent_config: template
  text:
  mask_token: <mask>
  num_token: 5
  placeholder_mapping: 
    <text_a>: text_a
    <text_b>: text_b
  prefix_dropout: 0.0
  mid_dim: 512
  optimize: 
    name: AdamW
    lr: 0.00005
    betas: 
      - 0.9
      - 0.999
    eps: 0.00000001
    scheduler: 
      num_warmup_steps: 0
  





