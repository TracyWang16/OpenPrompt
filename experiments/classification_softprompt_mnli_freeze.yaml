dataset:
  name: mnli
  path: datasets/TextClassification/mnli

plm:
  model_name: roberta
  model_path: roberta-large
  optimize:
    freeze_para: True
    lr: 0.0003
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 500

train:
  num_epochs: 30
  batch_size: 32

test:
  batch_size: 32

valid:
  batch_size: 32


template: soft_manual_template
verbalizer: manual_verbalizer


soft_manual_template:
  choice: 0
  file_path: scripts/TextClassification/mnli/soft_manual_template.txt
  optimize:
    freeze_para: True
    lr: 0.003
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 50

manual_verbalizer:
  choice: 0
  file_path: scripts/TextClassification/mnli/multiwords_verbalizer.jsonl
  optimize:
    freeze_para: True
    lr: 0.003
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 50
  
environment:
  num_gpus: 1
  cuda_visible_devices:
    - 2
  local_rank: 0 

learning_setting: full #few_shot

# few_shot:
#   parent_config: learning_setting
#   few_shot_sampling: sampling_from_train
  
# sampling_from_train:
#   parent_config: few_shot_sampling
#   num_examples_per_label: 10
#   also_sample_dev: True
#   num_examples_per_label_dev: 10
#   seed: 123

task: classification
classification:
  parent_config: task
  metric:  # the first one will be the main  to determine checkpoint.
    - accuracy  # whether the higher metric value is better.
  loss_function: cross_entropy ## the loss function for classification